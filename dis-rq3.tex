
\section{Result Discussion RQ3}
For the third and final research question, the objective was to assess how the AI generated emotion labels obtained through the speech-based and text-based emotion recognition would compare to the self-reported emotions provided by the interviewees.

In examining the alignment with the speech-based emotion labels from Hume AI and the text-based emotion labels from NLP Cloud with the self-assessed emotion scores, insightful findings revealed some levels of alignment dependent on both the model and emotion.

Figure~\ref{fig:comp-bar-rq3-all} builds on figure~\ref{fig:rq2_sent_grouped_bar}, discussed for RQ2. Presenting mean emotion scores for Hume AI and NLP Cloud, figure~\ref{fig:comp-bar-rq3-all} also introduced the self-assessed emotion scores.
Key differences were found between the models in comparison to the scores obtained from the interviewees, as NLP Cloud rated joy much higher than both the other modalities. As earlier discussed in section~\ref{subsec:RQ2interpretation}, this could still be explained by the spontaneous interview format in a calm setting, as this format may not encourage expressively conveying emotions, leading to the model interpreting textual language as joyful even though the tone may have been more neutral.
Suprise presented almost identical values for the self-assessed scores in comparison to NLP Cloud, although for all other emotions (anger, sadness and fear) the participants of the interviews rated their emotions almost an average between the two models, with the scores not being quite as high as Hume AI, and not as low as NLP Cloud. In the case of Hume AI estimating anger substantially higher than NLP Cloud and somewhat higher than the interviewees themselves, there might have been misinterpreted signs of anger coming from pitch and intensity during the interviews. These results suggest that the only emotion detected somewhat similarly here compared to the self-assessed scores, was surprise, whereas the self-scores landed on a middle ground between the two models for all other emotions in the positive oriented interviews.
Higher ratings of anger by NLP Cloud compared to the two other sources was likely due to the context of negative wording in the negative interviews forwarding the emotions more than was both felt by the participants and detected trough the voice.
NLP Cloud scored substantially closer scores to the self-assessed emotion scores for joy and sadness compared to Hume AI, suggesting the context of the words matched the emotions in the interviews better which indicates that the text-based model might have been better at capturing emotions from the vocal recordings. Hume may not have picked up vocal cues in the same capacity, likely due to the low expressions and more neutral speech during the interviews. This may further reflect a limitation in emotion recognition from spontaneous speech, which also has been highlighted in earlier research. Research done by \textcite{Cao2015}, found even advanced ranking-based classifiers which had outperformed traditional models, to struggle with neutrality in spontaneous speech \autocite{Cao2015}.
Further analysis for the positive clips showed Hume AI rated the emotion joy extremely high in comparison to the two other modalities. This further confirms what was earlier explained in the discussion for RQ2 ~\ref{subsec:RQ2interpretation}, that Hume AI may have incorrectly interpreted certain vocal cues as joy, for example nervous laughter or sarcasm, which could be difficult for a speech-based AI to recognize.

Overall for the sentiment-based comparisons, the negative recordings showed a better alignment for all three modalities. For both the positive and negatively oriented recordings, the two models performed better for some emotions as the text-based AI NLP Cloud seemingly captured the context for each interview more effectively for some emotions than the speech-based model and vice versa. This underscores the limitation of relying exclusively on either speech-based or text-based emotion recognition, as the different models capture different emotions with varying success. Using both models in comparison to the self-assessed scores gives a wider understanding of the performance for the text-based versus speech-based emotion recognition model and their different strengths and weaknesses. This aligns with earlier research which also have concluded that using more than one approach results in a better performance than only relying on an individual source \autocite{Cao2015}.

When examining the positive recordings and the negative recordings individually for the correlation analysis for Hume AI, no significant correlations were found for any emotions, although the positive recordings showed moderate positive trends for anger and joy. These emotions did not reach any significant correlations, but moderate r-values suggests a possible relationship that may be of interest to explore in the future with a lager dataset or different methods, as the lack of statistically significant correlations indicated that the emotions captured in the interviews do not align closely with those in Hume AI. 
For the correlation analyses for NLP Cloud, surprise was poorly detected with low correlations, suggesting the model struggled with the interpretation of the emotion surprise, possibly as a consequence of the complexity of the emotion and once again the nature of the interview format.

While NLP Cloud showed stronger correlations with the self-reported emotions overall compared to the Hume AI model, results showed that the model performs inconsistently across the different contexts (negative and positive), suggesting that a more consistent recognition of emotions may demand more modalities for better accuracy. These results also point to the fact that surprise remains a complex emotion with more challenges to capture from the data used in this study, although there are additional reasons as to this challenge. In the self-evaluation segment of the interviews, multiple participants expressed certain confusion regarding the assessment of the emotion surprise. A large part of the interviews consisted of describing past emotional experiences which may have reduced the intensity of surprise. Typically, surprise is expressed as an immediate reaction to unexpected events and itâ€™s unlikely that the interviewees are able to genuinely experience the same surprise felt in the original moment of the memory. This provides a possible explanation for why both AI models overall detected low levels of surprise, while an acted dataset could present higher correlations for this emotion. As stated in earlier research, acted speech is an amplification of emotions and spontaneous speech may lack the level of intensity to be distinguished from different emotions \autocite{Chakraborty2016}.

For the statistical analysis and effect sizes showed to be consistent with earlier findings, further giving grounds to this discussion. 
The full dataset showed Hume to have a moderate tendency for overestimation of the emotion anger in comparison to what the participants of the interviews had reported themselves, which remains true for the positive recordings where anger was very highly detected by Hume. As Hume also tends to underestimate surprise compared to the self-assessed scores, it is further confirmed that surprise is a difficult and complex emotion to detect. This validates conclusions from earlier research stating that neutrality is difficult for a model to deal with \autocite{Cao2015}. It is possible the neutrality of the speech often coming across in a spontaneous interview format may have been one of the reasons as to why emotions like surprise were detected at low levels. Hume AI as a speech-based emotion recognition model may not be capable of detecting subtle emotions and appears to struggle without the textual context as some vocal cues seems to have been misinterpreted.
No significant differences were found for NLP Cloud in the negative context, which further confirmed a closer alignment for NLP Cloud and the self-assessed scores in the negative contexts as earlier discussed in this chapter.
In the cases of where the alignment for the models and self-assessed scores did not align as well, further explanations can be drawn from earlier research stating sentiment do not inevitably display themselves in expressions or behaviors \autocite{Soleymani2017}. In many cases both Hume AI or NLP Cloud overestimated or underestimated scores compared to the self-assessed values, likely because of vocal expressions not always aligning with the internal affective states as sentiment is not always fully articulated.
