\section{Conclusions}
\subsection{Conclusion for RQ1}

\subsection{Conclusion for RQ2}
In answering RQ2, this study explored correlations of speech-based emotion recognition through Hume AI and text-based emotion recognition through NLP Cloud. 

The comparison presented patterns in how the different AI models interpreted the positively oriented interviews versus the negatively oriented interviews, where partial agreements were found between the two AI models for some particular emotions such as anger and joy for all recordings, joy and sadness for the positive recordings, and only joy for the negative recordings. Notable disagreements were present for fear and surprise, and statistical tests confirmed a significant difference for specifically fear.

Hume rated joy unexpectedly high in the negative interviews, where a possible reason could be nervous laughter or other emotions that could have been misclassified due to the interview format. NLP Cloud showed higher values of anger and sadness in the negative interviews, likely due to being able to better capture the negative context of the interviews through the text-based analysis.

Overall, the results underscores that the two AI models overall differ in the job of emotion detection, possibly due to the vocal recordings involving subtly expressed emotions or possible vocal cures. For example, irony or nervous laughter could have been incorrectly categorized as joy. These findings brought attention to the challenges of detecting the emotional cues for more complex emotions, whereas the emotional cues may be too subtle, as well as the importance of not relying on one modality as there might be some limitations in the models for detection of complex and subtle emotions.

\subsection{Conclusion for RQ3}
For RQ3, the comparison of the models with the self-assessed emotion scores indicated a stronger consistency for NLP Cloud than it did for Hume AI.
Overall, surprise was presented as an emotion consistently challenging to detect across the models, possibly due to the complexity of the emotion combined with the nature of the interviews where this emotion may have been expressed the least.

The results for this study suggest that a multimodal approach integrating multiple sources could enhance the precision and reliability of the detection systems for emotions, as relying on only one type of analysis may not be enough for the complexity of emotions.
