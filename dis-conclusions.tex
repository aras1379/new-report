\section{Conclusions}
\subsection{Conclusion for RQ1}
The result for the first research question has investigated if AI-based emotion recognition models align with existing research on vocal markers with a focus on the Swedish language.
Exploring vocal markers correlation with Hume AI emotion labels as well as the correlation between vocal markers and Praat, the overall result demonstrated limited strength.
Overall the results showed weak or moderate correlations, although some relevant patterns aligning with the existing research on Swedish vocal markers \autocite{Ekberg2023} was found.
While the values from the Hume correlations appeared moderate at best, the Praat correlations overall showed weak correlations except some misleading numbers suggesting an over-reliance on the vocal markers pitch and HNR.

The segment level analysis gave important insight into the fluctuations in the emotions throughout entire clips compared to an average value of the different emotions.

The result indicates that while there are certain vocal features that remain relevant as indicators of emotional states, spontaneous speech presents challenges in emotion recognition. In comparison to acted datasets, emotions are more subtle with more variety and contextual dependence in spontaneous speech.
Though there are challenges with spontaneous speech, the result suggest that AI-based emotion recognition systems such as Hume AI showed promise, demonstrating some flexibility and context-awareness.

Future work could benefit from incorporating a wider range of vocal features, emotions and a more dynamic approaches to capture the complexity of emotional expression.

\subsection{Conclusion for RQ2}
In answering RQ2 and RQ3, this study explored effectiveness of speech-based emotion recognition, Hume AI, text-based emotion recognition, NLP Cloud. These were later examined for potential alignments with self-assessed scores for emotions.
For RQ2, partial agreements were found between the two AI models for some particular emotions such as joy and anger, though notable disagreements were present for fear and surprise. 

This brought attention to the challenges of detecting the emotional cues for more complex emotions, where they might have more subtle cues for detection.
In comparisons between the models, certain differences in how each model captured emotions were found.

\subsection{Conclusion for RQ3}
For RQ3, the comparison of the models with the self-assessed emotion scores indicated a stronger consistency for NLP Cloud than it did for Hume AI.
Overall, surprise was presented as an emotion consistently challenging to detect across the models, possibly due to the complexity of the emotion combined with the nature of the interviews where this emotion may have been expressed the least.

The results for this study suggest that a multimodal approach integrating multiple sources could enhance the precision and reliability of the detection systems for emotions, as relying on only one type of analysis may not be enough for the complexity of emotions.
