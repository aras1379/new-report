\section{Conclusions}
\subsection{Conclusion for RQ1}
The result for the first research question has investigated if AI-based emotion recognition models align with existing research on vocal markers with a focus on the Swedish language.
Exploring vocal markers correlation with Hume AI emotion labels as well as the correlation between vocal markers and Praat, the overall result demonstrated limited strength.
Overall the results showed weak or moderate correlations, although some relevant patterns aligning with the existing research on Swedish vocal markers \autocite{Ekberg2023} was found.
While the values from the Hume correlations appeared moderate at best, the Praat correlations overall showed weak correlations except some misleading numbers suggesting an over-reliance on the vocal markers pitch and HNR.

The segment level analysis gave important insight into the fluctuations in the emotions throughout entire clips compared to an average value of the different emotions.

The result indicates that while there are certain vocal features that remain relevant as indicators of emotional states, spontaneous speech presents challenges in emotion recognition. In comparison to acted datasets, emotions are more subtle with more variety and contextual dependence in spontaneous speech.
Though there are challenges with spontaneous speech, the result suggest that AI-based emotion recognition systems such as Hume AI showed promise, demonstrating some flexibility and context-awareness.

Future work could benefit from incorporating a wider range of vocal features, emotions and a more dynamic approaches to capture the complexity of emotional expression.

\subsection{Conclusion for RQ2}
In answering RQ2, this study explored correlations of speech-based emotion recognition through Hume AI and text-based emotion recognition through NLP Cloud. 

The comparison presented patterns in how the different AI models interpreted the positively oriented interviews versus the negatively oriented interviews, where partial agreements were found between the two AI models for some particular emotions such as anger and joy for all recordings, joy and sadness for the positive recordings, and only joy for the negative recordings. Notable disagreements were present for fear and surprise, and statistical tests confirmed a significant difference for specifically fear.

Hume rated joy unexpectedly high in the negative interviews, where a possible reason could be nervous laughter or other emotions that could have been misclassified due to the interview format. NLP Cloud showed higher values of anger and sadness in the negative interviews, likely due to being able to better capture the negative context of the interviews through the text-based analysis.

Overall, the results underscores that the two AI models overall differ in the job of emotion detection, possibly due to the vocal recordings involving subtly expressed emotions or possible vocal cures. For example, irony or nervous laughter could have been incorrectly categorized as joy. These findings brought attention to the challenges of detecting the emotional cues for more complex emotions, whereas the emotional cues may be too subtle, as well as the importance of not relying on one modality as there might be some limitations in the models for detection of complex and subtle emotions.

\subsection{Conclusion for RQ3}
In conclusion for RQ3, the comparison of the models with the self-assessed emotion scores indicated a stronger consistency for NLP Cloud than it did for Hume AI.

The results suggested that the accuracy of the AI models in emotion detection is not consistent across the emotions, although the challenging nature of self-assessing emotions, especially for fear and surprise, retrospectively possibly complicates this process, effecting the results as well.
Overall, surprise was presented as an emotion consistently challenging to detect across the models, possibly due to the complexity of the emotion combined with the nature of the interviews where it is highly possible that this emotion may have been expressed the least.

The results for this study suggest that a multimodal approach integrating multiple sources could enhance the precision and reliability of the detection systems for emotions, as relying on only one type of analysis may not be enough for the complexity of emotions.
