\section{Method Discussion}
\subsection{RQ1 Methodological Considerations}
To answer RQ1, the methodological approach involved analysis of emotional expression for vocal markers in Swedish speech in comparison to AI based emotion recognition models. The idea was to analyze emotions in a clip in its entirety and find correlations, which had some differing results, but it proved to be a notable strength to execute the analysis on a segment level to capture emotional fluctuations in a more dynamic way. While this offers another another perspective, this approach introduced challenges of its own in having some inconsistent emotions not aligning completely across the segments. Therefore the methodological approach was partially fulfilled for answering RQ1 by identifying some emotional fluctuations, while also revealing challenges in both the analyses for segment-leveled clips and full clips.

One of the studies chosen to compare the results with, being the existing Swedish emotion research by Ekberg (Ekberg et al., 2023) proved some similarities and patterns which provided valuable information to this study.  However, the research used pre-defined sentences, repeated by actors, which may have given a more consistent result than the dataset used in this research which consisted of interviews capturing spontaneous speech. With 16 participants, the dataset resulted in a total of 32 recordings across a diverse group of participants consisting of men and women with ages ranging from the 23-78. The spontaneous speech and large variety of interview questions combined with dataset size may indicate some limitations for the result. While RQ1 was addressed, a larger and more controlled dataset with acted emotions along with repeated sentences, could possibly have validated some observed patterns, ensuring more consistent emotions throughout the recordings.

Hume AI was one of the models used and provided some advantages such as avoiding manual labeling and being pre-trained, although the Hume AI emotion scores had to be normalized and the emotions were filtered to use only the specific five emotions necessary for the comparisons in this research, which may have had some limitations on the model’s capacity. Along with working well for the research’s purpose, the model has some downsides. For example, there is limited publicly available information about functions of the model, making it difficult to fully assess possible limitations and biases.

Despite these limitations, Hume AI contributed with valuable insights in answering RQ1. 

A set of basic vocal features which consisted of pitch, intensity, harmonic-to-noise ratio (HNR), jitter, shimmer, was extracted through Praat. These features are well established indicators of emotional expression but proved to be somewhat of a limitation which possibly could have been avoided by incorporating additional vocal features. Given that the dataset for this research consisted of interviews capturing spontaneous speech, a broader range of vocal features might have contributed to the detection of the complex vocal patterns and given a more nuanced understanding of the correlations for emotion recognition in the Swedish language.

While the selected vocal markings chosen for this study gave some insight into addressing RQ1, expanding the set of features could have helped address RQ1 more comprehensively. 

\subsection{RQ2 and RQ3 Methodological Considerations}
The methodological approach to address RQ2 combines analysis of transcribed text in emotion recognition using NLP Cloud in order to assess the emotional content of speech transcripts in relation to speech-based AI models.

In addressing RQ3, the approach was to compare self-reported emotions with the AI-generated labels from both speech and text-based models to analyze potential alignments.
This is a multi-modal approach with several methodological considerations, but also some methodological strengths.

The vocal recordings were transcribed and analyzed with the text-based emotion recognition tool NLP Cloud and self-assessments of emotions were collected after each interview, allowing a comparison between speech-based AI, text-based AI and self-assessment scores given by the participants in the interviews. 
The use of three different methods resulted in triangulation, which increased the flexibility and credibility in the findings. In addition to this, the usage of pre-trained AI models ensured consistent processing. However, some information loss was expected for the transcript text analyzed in NLP Cloud. When the model takes in what is said rather than how it is said, many important emotional cues such as intensity or pitch get lost. This could possibly have led to some emotions being misinterpreted or not catching the full complexity of the emotions expressed, based on only the text-based analysis.

While NLP Cloud contributed in addressing RQ2, some limitations in loss of prosodic information may have reduced the full emotional understanding.
The self-reported emotions introduced a valuable reference point for this research. Some agreement was found between the AI models and self-reported emotions, but some of the self-assessed scores may also have been slightly exaggerated. The emotional memories and personal interpretations of emotions by participants can have influenced the self-assessed emotion scores.
While the self-reported emotion scores have some limitations which likely contributed to some variability in the analyses, they helped valuably address RQ3.

The complexity of emotion detection across different modalities is highlighted by the AI models being able to capture some emotions in a quite robust way while struggling more with others. The few emotional categories used for this research may have limited the emotion recognition, where an implementation of more emotions and features possibly could have captured the emotions in a better way.

All methods used in answering RQ2 and RQ3 provided valuable information and findings, however the research could have benefited from an expansion of the emotion categories to help identify emotions in a more accurate and complex way.

\subsection{Summary of Methodological Considerations}
To address all research questions, this study utilized a multi-method approach combining speech-based and text-based emotion recognition with self-reported emotion scores of the participants from the interviews. 

Although all methods contributed with important findings and significant insights into emotional expression in Swedish speech, a number of limitations emerged.

While the triangulation of speech, text and self-assessment scores contributed to the strength and credibility of the findings, size of dataset, model transparency and other limitations such as variabilities and inconsistency in having spontaneous interviews may have impacted the effectiveness of the findings. Although highlighting some areas for improvement for future studies, the methods chosen for this research overall contributed to answering the research questions in a comprehensive manner.
