\section{Method Discussion}

The methodology to collect data through interviewing people with the intention to evoke emotions to analyse the recorded data have limitations, even if beneficial to the purpose of the study where real-world speech should be analysed in terms of expressed emotions. The interviews were designed to provoke one positive and one negative emotion, yet not directly oriented towards one of the three negatively oriented emotions. This structure was motivated by allowing the participants to talk freely about a subject they related to and felt comfortable to discuss. During the self-report after each interview, several participants raised confusion about how to interpret and rate surprise. This emotion showed most inconsistency throughout all research questions and was not aimed to be induced through the interviews. By reducing this study to only focus on one positive and one negative emotion, both for interview design and in data analysis, a more comprehensive analyse of only these emotions could be conducted to yield a narrower, yet deeper analysis of two focus emotions. 

\medskip
Regarding the interviews, its setting cannot be perceived as a clear representation of real conversational speech, due to its reflective nature. The first research question where vocal markers based on previous research on Swedish vocal markers are based on based on pre-defined sentences, repeated by four actors, contrasting our analysed dataset. The spontaneous speech and large variety of interview questions combined with dataset size may indicate some limitations for the result. During the data analysis it was found that pitch and HNR are significantly diverged for male and females [APPENDIX]. This has most likely impacted the results for all analyses conducted on the full dataset, by the reason that these features even out by the other gender and may create a normalised average even if it would be distinct if analysed separately. The utilized research for the rule-based categorisation function and to evaluate similarities between researched vocal features and emotional output from speech recognition included a larger number of vocal features than is used for this study. 

\medskip
Comparing the results with the prior study on Swedish vocal markers indicate some similarities and patterns providing valuable information to this study, even if several correlations and patterns remain weak. Mainly focusing on this study as reference may create bias, by the reason that there is vastly limited prior research on Swedish in this field. Additionally, this study employed a larger number of vocal features than extracted for these results. The three frequency formants were included in both categorisation functions yet excluded in the overall data analysis. Beyond these, the Swedish study included 14 additional features, some of these were not possible to extract with Praat Parselmouth and therefore excluded. Voiced- and unvoiced length were not relevant for this study since the recordings were edited, including deleting some silent moments. Not including the full set of features is a clear limitation for the results. While the selected vocal markings chosen for this study gave some insight into addressing RQ1, expanding the set of features could have helped address RQ1 more comprehensively. The interviews did not have a clear timeframe, resulting in varying length of the recordings even after editing. This could have been stricter and more planned to maintain consistency across all recordings. 
The initial idea was to analyse emotions in a clip in its entirety and find correlations, including average vocal markers and Hume probabilities, which due to differentials in expressions during an interview most likely contributed to even-out values. Time-segmenting proved to have a notable strength to execute the analysis on a segment level to capture emotional fluctuations in a more dynamic way. Output from Hume is pre-segmented with frequency variation of 1-4 seconds, single clip dependent. Vocal features were segmented into 2.5 second timeframes used for all clips in the general data analysis, leading to divergencies in segment length. Therefore, we tested how changing the segmented time with 1 second for selected individual clips, yielding in both higher and lower results, depending on the clip. For more accurate results, it should have been adjusted separately for each clip before segmented analysis to match the time frames extracted from Hume. The methodological approach for the RQ1 was partially fulfilled by identifying some vocal fluctuations in emotions, while also revealing challenges in both segment-level and average values. Further insights could have been provided with utilizing the rule-based functionality for emotion grouping segment wise as well, to find if grouped features could provide higher correlation results for Hume labelling and acoustic markers. 
Hume AI was one of the models used and provided some advantages such as avoiding manual
pre-training, although the Hume AI emotion scores had to be normalized and the emotions were filtered to use only the specific five emotions necessary for the comparisons in this research, which may have had some limitations on the model’s capacity. Along with
working well for the research’s purpose, the model has some downsides. For example, there is limited publicly available information about functions of the model, making it diﬀicult to fully assess possible limitations and biases. Despite these limitations, Hume AI contributed with valuable insights in answering RQ1.

\medskip
Comparing our interview-based result with a larger and more controlled dataset with acted emotions could possibly have validated some observed patterns or strengthened the opposite whereas acted speech is expressed explicitly different from speech in real-world similar contexts. This is also relevant for the second research question extending to text-based emotion recognition with NLP Cloud that analysed the transcript from the same recordings as RQ1. However, collecting our own data was beneficial for the third research question where the AI-models output was compared to self-assessed emotion scores. 
Utilizing NLP Cloud have limitations. To be able to decide on which emotions could be included in the output, prompt-based usage was necessary. This was a zero-shot prompt without deeper examination, which potentially lead to inconsistent results. The prompt should have been tested more comprehensive and verified through analysing the same transcription several times to observe the outputs consistency. 
The self-reported emotions introduced a valuable reference point for this research. Some agreement was found between the AI models and self-reported emotions, but some of the self-assessed scores may also have been slightly exaggerated. The emotional memories and personal interpretations of emotions by participants can have influenced the self-assessed emotion scores.
While the self-reported emotion scores have potential limitations and contributed to some variability in the analyses, they helped valuably address RQ3. The complexity of emotion detection across different modalities is highlighted by the AI models being able to capture some emotions in a quite robust way while struggling more with others. In this context, it could have been beneficial to include a larger number of emotions that are more relevant to interview circumstances. However, most research include around five to six emotions, including the selected for this study. 

\medskip
The multi-method approach combining speech- and text-based emotion recognition with self-reported emotion scores of the participants from the interviews contributed to insights into emotional expression in Swedish speech, despite several limitations. A narrower approach could have been constructive in terms of deeper analysis. It could also have been useful to study a larger dataset and compare speech from conversational interviews, actual real conversations and acted datasets to gain an understanding of how much acted datasets can impact speech recognition overall.  
While the triangulation of speech, text and self-assessment scores contributed to the strength and credibility of the findings, size of dataset, model transparency and other limitations such as variabilities and inconsistency in having spontaneous interviews may have impacted the effectiveness of the findings. Although highlighting some areas for improvement for future studies, the methods chosen for this research overall contributed to answering the research questions in a comprehensive manner.
