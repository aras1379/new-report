\section{Contribution to the Field}
\label{sec:con-contribution}
This thesis contributes to the vocal and linguistic aspect of the growing fields affective computing and natural language processing (NLP).

This study utilized a multimodal approach, using both text and audio. The results of this suggested that using more than one modality improves the accuracy when classifying emotions.

While there is a lot of existing research focusing on vocal emotion recognition in affective computing and NLP, there was a noticeable gap for research specifically in the Swedish language. 
Much of the existing research also used acted datasets, which was not used in this study. Instead, this study consisted of semi-structured interviews with a set of questions for the interviewees to choose from. With the spontaneous nature of the datasets from speech recorded from interviews, this offers valuable insight into how emotions can be recognized in a setting more similar to real life. This can also contribute to the development of more emotionally aware AI models and systems through the insights of the more subtle cues for different emotions in these settings. Possible areas where this can be applied is in human-computer interactions, virtual assistance, and mental health monitoring or similar.
