\section{Result Discussion RQ1}

For the first research question, this thesis investigated how an AI model for speech recognition
compare to existing research on vocal markers. More specifically, the goal is to assess whether
the AI models align with the findings of the Swedish research on vocal markers done by \textcite{Ekberg2023}. 

The attempts to categorise emotions based the results on this Swedish study cannot be directly evaluated in terms of how accurate the function yielded emotion labels. Even if the standardised function had better recall against Hume, the divergences between the scores were minimal. 
The rule-based version got higher recall in addition of feature-based adjusting compared to Hume, which is not a ground truth, still beneficial for relative comparison. 
The improvement is aligned to prior research \autocites{Banse1996}{Ekberg2023}, minimising the extreme values for joy according to the wide acoustic markers while increasing the weight of pitch, which is stated to be the most prominent feature perceptually. 
Restricting anger to be required two of its studied characterises lead to slightly fewer recordings being top labelled as anger, which was added by the reason to separate anger from happiness. The final version presented some alignment between the top emotions, negative recordings yielded higher correlation where both models rated anger highest for six clips and sadness for two clips. More confusion occurred for the positive recordings. 
The interview setting is likely a contributor to this, where positive oriented questions can be answered with the same tone as negative evoked answers. 
Fear and surprise were rarely labelled by both methods, Hume distribution is presented in more detail for RQ2. 
However, even if the results yielded higher recall against Hume, it cannot be benchmarked that these alternations resulted in exactly the emotions that were expressed – a complex question to answer no matter what it is compared to, due to the nature of abstract perception of expressed emotional state, both internally and externally. When formulating the rule-based categorisation function, subjective opinions have an impact about fear and surprise rarely being expressed in the interviews overall, 
by the reason that the interviews were not designed to evoke these emotions which is complicated to recall in interview circumstances, especially for surprise. 

Comparison of vocal features against Hume’s emotions probability and our categorisation labelling demonstrates that the methods label emotions based on different data. Only sadness shows a similar vocal pattern between the sources, both negatively correlated with pitch and HNR, for other features, not as strong correlation similarities occur. 
No other emotion shows similarities, presumably by the reason that Hume has a much more advanced approach to recognising emotions than only using a few vocal features as indicators, as explained in Theoretical Framework~\ref{sec:vocal-markers}.  The fact that average values for a whole recording was used for these analyses, are most likely impacting the results due to low-expressed vs high-expressed segments even out.  
The subjective possible impact is a clear limitation as well as utilizing the Swedish research as a foundation for this categorisation since it is based on a small, acted dataset whereas this study analyses semi-structured interviews with spontaneous speech in conversational interview format. Limitations for the categorisation function also involves alternations that, even if motivated by prior research, might not yielding fully accurate results. 
Therefore, the remaining data analysis include Hume's probability and vocal features, not categorised manually.

To answer the research question, Time-to-Time analysis reveal what occurred for sadness in the comparison between vocal features and both emotion categorisation methods. Highest correlation values together with shifts for high-emotion occurrence with certain features resulted in sadness being most prominent predicted when pitch and HNR was lower, but also when intensity decreased. 
All three relationships are aligned to the Swedish research, where these acoustic markers have lower mean than other emotions. 
Fear was the second emotion with higher correlations, for example being predicted when pitch and HNR was raised, also according to the utilized research results. Joy had the highest intensity shift, that is stated in the research. However, anger had lower results for intensity which contrasts with expectations. 
When interpreting these results, it is important to acknowledge that the Swedish research do not specify how strongly the emotions are expressed, it is hard to define if anger is perceived as screaming or as a negative tone. 
Again, the nature of emotions is complex, and it is not possible to compose if the results are good or bad, which was not the purpose of this study. Conducting time segmented analyses generated additional insights that reduced the limitation with using mean values from the recordings, revealed by observing feature shifts. 


Despite that less than half of the feature-emotion group was significant, with most correlations being weak, certain patterns aligned with theoretical expectations was disclosed. The case examples provide a visualisation on both how certain vocal features align with Hume-predicted emotions, presenting similar patterns as prior research. It also shows how the fixed time-segments can have a negative impact on the correlation values, since Hume have clip varying time frames, the segments are not fully aligned.  
While correlations between AI-labelling and vocal markers are generally weak to moderate, it is important to acknowledge that Hume emotion scores should not be perceived as perfect estimates of true emotional states, even if it probably results in reasonable predictions. 
These analysed recordings consist of spontaneous and conversational speech which likely do not involve as strong emotional expressions as acted datasets, with tendency of more subtle and reduced level of vocal features. The simplicity regarding the number of applied vocal markers in contrast to the emotion-trained AI model reaching beyond the use of single acoustic features is a certain reason that this study should be interpreted as explanatory and not to benchmark the general efficiency of emotion recognition in Swedish speech.    

The wide acoustic spread for certain emotions, as anger and joy, presented in both \autocites{Banse1996}{Ekberg2023} and was prominent when adjusting the rule-based emotion categorisation where alternations had to be included to seperate them from each other. 
This imply that vocal-marker theory can be limited in conversational context and the intention of the speaker, such as sarcasm and genuine versus polite emotions. 

Certain vocal patterns do in fact recognize expressed emotions, as we as humans can interpret certain emotions in others through their speech, algorithms and technology can do the same, at least to some extent. 
Even if some Hume-labelled emotions align with research on acoustic markers, more than these features are utilized to recognise emotions with AI. The fact that some alignment arise between the Swedish research and Hume AI, 
suggests that the language does not necessary have an overly negative impact, yet our results are not comprehensive enough to confirm this. 